{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-triana/MEPs/blob/main/tweets_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FUgsEmtomjBP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Wed Jul 27 11:50:55 2022\n",
        "@author: Daniel Triana\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyreadr\n",
        "!pip install deep_translator"
      ],
      "metadata": {
        "id": "1QkmWLSXawhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated translation using Python\n",
        "This is the .ipynb version of the Python script for automated translation. <p>\n",
        "This script is built upon the Deep-Translator tool created by Nidhal Baccouri: \n",
        "https://pypi.org/project/deep-translator/"
      ],
      "metadata": {
        "id": "Jh3HjSAnoJpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Import relevant packages\n",
        "from typing import Union, Any\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "from pandas.core.generic import NDFrame\n",
        "from pandas.io.parsers import TextFileReader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pyreadr\n",
        "import deep_translator\n",
        "import deep_translator.base\n",
        "import deep_translator.exceptions\n",
        "from deep_translator import GoogleTranslator, single_detection, batch_detection\n",
        "import requests\n",
        "import time"
      ],
      "metadata": {
        "id": "UE8ZGzCPpgQj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "# Load your Google Drive in case you have your files stored there \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_y85zpabGzu",
        "outputId": "094a8170-36a5-4ea7-dd29-ec28bd609134"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Data Base\n",
        "To work with the database, I've created a simplified .csv version of the full .Rda original file. Beware that while working in Colab, you'll need to upload the .csv file every time you start the Colab Notebook."
      ],
      "metadata": {
        "id": "AHiqNO6lqKkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Load the DataBase\n",
        "tweets_text: Union[Union[TextFileReader, DataFrame], Any\n",
        "                   ] = pd.read_csv(r'/content/drive/MyDrive/tweets_text.csv', low_memory=False)\n",
        "tweets_text[['true_author_id', 'id', 'conversation_id', 'commission_dummy',\n",
        "             'party_id', 'in_reply_to_user_id'\n",
        "             ]] = tweets_text[['true_author_id', 'id', 'conversation_id',\n",
        "                               'commission_dummy', 'party_id',\n",
        "                               'in_reply_to_user_id'\n",
        "                               ]].astype(str)\n",
        "\n",
        "# Visualize the first 5 rows of the data base.\n",
        "tweets_text.head()"
      ],
      "metadata": {
        "id": "LqICnezerZXA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'language_codes.csv' file is included as a reference for the international language codes. Although uncommon, the Twitter language codes within the database can differ from this reference. It is advised to check the correct database code for the language we want to translate.<p>\n",
        "Don't forget to load-up the file everytime you open the Notebook."
      ],
      "metadata": {
        "id": "SHZPtsiyrvaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File with the international language codes for reference\n",
        "lang_codes = pd.read_csv(r'/content/drive/MyDrive/language_codes.csv')\n",
        "lang_codes"
      ],
      "metadata": {
        "id": "AVmyeI3WroFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are going to translate slices of specific languages, weÂ´ll need to know how many tweets per language are in the database."
      ],
      "metadata": {
        "id": "s-J4UbIyuBKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create object to know how many tweets per language are in the DataBase\n",
        "tweets_per_language = (tweets_text['lang'].value_counts())\n",
        "tweets_per_language"
      ],
      "metadata": {
        "id": "1Ud9OToOukjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter the Language Subset\n",
        "- Filter or slice the relevant set of tweets to be translated. <p>\n",
        "- The .iloc method is used to slice the translation subset within the language subset. <p>\n",
        "- The recommended batch length is between 2000 and 3000. <p>\n",
        "Important note: Google server will shut down your IP Address if you try to translate a massive number of tweets at the same time."
      ],
      "metadata": {
        "id": "U33TDT5EvhRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# 'de' is the language code for German (source language).\n",
        "# german = tweets_text.query('lang ==\"de\"')\n",
        "# Create a new data frame for every batch or subset.\n",
        "# Identify every subset by source language and batch number.\n",
        "german_2 = tweets_text.query('lang ==\"de\"').iloc[4240:6000]\n",
        "german_3 = tweets_text.query('lang ==\"de\"').iloc[6001:8001]\n",
        "german_4 = tweets_text.query('lang ==\"de\"').iloc[8002:10002]\n",
        "# ..."
      ],
      "metadata": {
        "id": "byJTIqF3vbRa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to create an empty data frame to be populated with the original text and the translated text. <p>\n",
        "Although this is not a necessary condition for the translation process, it will help us with version control."
      ],
      "metadata": {
        "id": "xXtzeVyH0F8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Generate empty dataframe with the columns \"text_original\" & \"text_translated\"\n",
        "# Create a new data frame for every translation batch.\n",
        "# Associate every data frame with its corresponding batch number.\n",
        "df_Transl_2 = pd.DataFrame(columns=['text', 'text_translated'])\n",
        "df_Transl_3 = pd.DataFrame(columns=['text', 'text_translated'])\n",
        "df_Transl_4 = pd.DataFrame(columns=['text', 'text_translated'])\n",
        "# ..."
      ],
      "metadata": {
        "id": "TyqZRMmoylhr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation Process"
      ],
      "metadata": {
        "id": "PdxCT8GU107D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# %%\n",
        "# for loop, translation process\n",
        "print('Beginning translation...')\n",
        "start = time.time()\n",
        "\n",
        "# Use the right batch to translate.\n",
        "for i, tweet in enumerate(german_3.text):\n",
        "    if str(tweet) == 'nan':\n",
        "        print('Reading task completed')\n",
        "        break\n",
        "    translation = GoogleTranslator().translate(text=tweet)\n",
        "    a = 1\n",
        "\n",
        "    # In case of no success, retries up to six times\n",
        "    while tweet == translation:\n",
        "        print('Could not translate the row ' + str(i) + ', retry ' + str(a))\n",
        "        translator = GoogleTranslator(service_urls=[\n",
        "            'translate.google.com',\n",
        "            'translate.google.de',\n",
        "            'translate.google.co.uk',\n",
        "            'translate.google.co.kr',\n",
        "            'translate.google.com.ec',\n",
        "            'translate.google.com.mx',\n",
        "            'translate.google.com.uy',\n",
        "            'translate.google.cn'\n",
        "        ])\n",
        "        translation = GoogleTranslator().translate(text=tweet)\n",
        "        a += 1\n",
        "        if a > 6: break\n",
        "\n",
        "    # Check if the text was translated\n",
        "    if tweet == translation:\n",
        "        print('Translation attempted on: ' + str(tweet) + ' Returned: ' + str(translation))\n",
        "    print(i)\n",
        "    # Populate Data Frame with the original text and the translation\n",
        "    df_Transl_3.loc[i] = [tweet, translation]\n",
        "print('... Task completed.')\n",
        "end = time.time()\n",
        "print(\"The time of execution is: \", end-start)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QO1QEl1kmjBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the translations\n",
        "We need to merge the translations with the subset data frame for every batch. <p>\n",
        "Suggestion: Check and double-check that nothing funky is going on with the translations and that everything is in the right place. e.g. Duplicated tweets, missing tweets, translations not matching the original text, etc."
      ],
      "metadata": {
        "id": "H48_c84c2yKC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": [
        "# Merge the DataFrames in order to have the translations in the same DataFrame\n",
        "# german_2 = pd.merge(german_2, df_Transl_2, on='text')\n",
        "german_3 = pd.merge(german_3, df_Transl_3, on='text')\n",
        "# german_4 = pd.merge(german_4, df_Transl_4, on='text')\n",
        "# ...\n",
        "german_3.head()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NiyNdmQkmjBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Change the order of the columns to get a better visualization of the data.\n",
        "Just write the column names in the order you want them to appear in the data frame."
      ],
      "metadata": {
        "id": "0RhQ0CTn3wWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": [
        "#%%\n",
        "# Change the order of the DF columns for ease of comparison.\n",
        "german_3 = german_3[['true_author_id', 'name', 'username', 'day', 'month',\n",
        "                     'year', 'dob', 'full_name', 'sex', 'country', 'nat_party',\n",
        "                     'nat_party_abb', 'eu_party_group', 'eu_party_abbr',\n",
        "                     'commission_dummy', 'party_id', 'engparty', 'party',\n",
        "                     'eu_position', 'lrgen', 'lrecon', 'galtan',\n",
        "                     'eu_eu_position', 'eu_lrgen', 'eu_lrecon', 'eu_galtan',\n",
        "                     'lang', 'text', 'text_translated', 'id',\n",
        "                     'public_metrics.retweet_count',\n",
        "                     'public_metrics.reply_count', 'public_metrics.like_count',\n",
        "                     'public_metrics.quote_count', 'conversation_id', 'source',\n",
        "                     'in_reply_to_user_id', 'geo.place_id',\n",
        "                     'geo.coordinates.type','created_at'\n",
        "                     ]]\n",
        "german_3.head()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TegLRKS3mjBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the data into a .csv file for storage purposes."
      ],
      "metadata": {
        "id": "yniMje3Q5gNq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "source": [
        "#%%\n",
        "# Save file\n",
        "german_3.to_csv('german_3_transl.csv', index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Bq6v5T-gmjBU"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "tweets_translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}